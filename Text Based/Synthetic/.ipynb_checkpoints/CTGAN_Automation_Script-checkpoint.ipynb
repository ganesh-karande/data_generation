{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "TSwvnnyvvrpQ",
        "outputId": "6887252d-ee4d-4324-c039-4b8337550631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from /content/BMW sales data (2010-2024) (1).csv...\n",
            "\n",
            "--- Available Columns in the Dataset ---\n",
            "['Model', 'Year', 'Region', 'Color', 'Fuel_Type', 'Transmission', 'Engine_Size_L', 'Mileage_KM', 'Price_USD', 'Sales_Volume', 'Sales_Classification']\n",
            "----------------------------------------\n",
            "\n",
            "Data loaded successfully. Shape: (50000, 11)\n",
            "Source: Auto-detected\n",
            "Categorical Columns being used (6): ['Model', 'Transmission', 'Sales_Classification', 'Fuel_Type', 'Color', 'Region']\n",
            "ACTION: Input data is newer than the existing model. Retraining is required.\n",
            "Starting CTGAN model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gen. (-2.67) | Discrim. (-0.03):  32%|███▏      | 97/300 [16:07<33:45,  9.98s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3854757269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# NOTE: Before running, ensure you have an 'input_data.csv' file in the same directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mrun_automation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3854757269.py\u001b[0m in \u001b[0;36mrun_automation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretrain_needed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Step 1: Train the model on the new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mctgan_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# Step 1: Load the existing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3854757269.py\u001b[0m in \u001b[0;36mtrain_and_save_model\u001b[0;34m(data, model_path)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Fit the model to the real data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCATEGORICAL_COLUMNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# Save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ctgan/synthesizers/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ctgan/synthesizers/ctgan.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, discrete_columns, epochs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                     \u001b[0moptimizerD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m                     \u001b[0mpen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m                     \u001b[0mloss_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0moptimizerD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from ctgan import CTGAN\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the file paths for input data, the saved model, and output data.\n",
        "INPUT_DATA_FILE = '/content/BMW sales data (2010-2024) (1).csv'\n",
        "MODEL_FILE = 'ctgan_model.pkl'\n",
        "SYNTHETIC_OUTPUT_FILE = 'synthetic_data.csv'\n",
        "\n",
        "# Define the categorical columns in the dataset.\n",
        "# IMPORTANT: Leave this list EMPTY [] for automated detection based on data type and cardinality.\n",
        "# If you need specific columns, list them here.\n",
        "CATEGORICAL_COLUMNS = [\n",
        "]\n",
        "\n",
        "# --- Core Functions ---\n",
        "\n",
        "def detect_categorical_columns(data: pd.DataFrame) -> list:\n",
        "    \"\"\"\n",
        "    Automatically detects columns that should be treated as categorical for CTGAN.\n",
        "\n",
        "    Heuristic:\n",
        "    1. Any column with dtype 'object' (string).\n",
        "    2. Any numeric column (int/float) with low cardinality:\n",
        "       - Less than 10 unique values AND\n",
        "       - Unique values are less than 5% of the total rows.\n",
        "    \"\"\"\n",
        "    detected_cols = []\n",
        "\n",
        "    # 1. Detect 'object' (string) columns\n",
        "    object_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    detected_cols.extend(object_cols)\n",
        "\n",
        "    # 2. Detect low-cardinality numeric columns\n",
        "    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
        "    n_rows = len(data)\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        n_unique = data[col].nunique()\n",
        "        # Criteria 1: Absolute count check (e.g., less than 10 unique values)\n",
        "        absolute_check = n_unique <= 10\n",
        "        # Criteria 2: Relative count check (e.g., less than 5% of total rows)\n",
        "        relative_check = (n_unique / n_rows) < 0.05\n",
        "\n",
        "        if absolute_check and relative_check:\n",
        "            detected_cols.append(col)\n",
        "\n",
        "    # Remove duplicates and return\n",
        "    return list(set(detected_cols))\n",
        "\n",
        "\n",
        "def load_data(filepath: str) -> pd.DataFrame or None:\n",
        "    \"\"\"Loads the input dataset from a CSV file and enforces categorical types.\"\"\"\n",
        "    global CATEGORICAL_COLUMNS # FIX: Moved global declaration to the start of the function\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        # Fallback check if the provided path starts with a path that might not exist locally\n",
        "        local_filepath = os.path.basename(filepath)\n",
        "        if os.path.exists(local_filepath):\n",
        "             filepath = local_filepath\n",
        "        else:\n",
        "            print(f\"ERROR: Input file not found at {filepath}\")\n",
        "            return None\n",
        "\n",
        "    print(f\"Loading data from {filepath}...\")\n",
        "    try:\n",
        "        data = pd.read_csv(filepath)\n",
        "\n",
        "        print(\"\\n--- Available Columns in the Dataset ---\")\n",
        "        print(data.columns.tolist())\n",
        "        print(\"----------------------------------------\\n\")\n",
        "\n",
        "        # Determine categorical columns (use manual list if provided, otherwise auto-detect)\n",
        "        if CATEGORICAL_COLUMNS:\n",
        "            active_categorical_cols = [col for col in CATEGORICAL_COLUMNS if col in data.columns]\n",
        "            all_cols = CATEGORICAL_COLUMNS\n",
        "            source_description = \"Manual list\"\n",
        "        else:\n",
        "            active_categorical_cols = detect_categorical_columns(data)\n",
        "            all_cols = active_categorical_cols\n",
        "            source_description = \"Auto-detected\"\n",
        "\n",
        "\n",
        "        missing_cols = [col for col in all_cols if col not in data.columns]\n",
        "\n",
        "        # --- Enforce categorical data types for CTGAN ---\n",
        "        for col in active_categorical_cols:\n",
        "            # Force the column to be treated as a Pandas Category type\n",
        "            data[col] = data[col].astype('category')\n",
        "\n",
        "        if missing_cols and source_description == \"Manual list\":\n",
        "            print(f\"WARNING: The following columns are in the MANUAL CATEGORICAL_COLUMNS list but MISSING from the data: {missing_cols}. Using only available columns.\")\n",
        "\n",
        "        if not active_categorical_cols:\n",
        "            print(\"CRITICAL ERROR: No categorical columns were found/defined. CTGAN requires at least one discrete or continuous column.\")\n",
        "            # Return None to prevent proceeding to model training with no features.\n",
        "            return None\n",
        "        # --------------------------------------------------\n",
        "\n",
        "        # Display information for verification\n",
        "        print(f\"Data loaded successfully. Shape: {data.shape}\")\n",
        "        print(f\"Source: {source_description}\")\n",
        "        print(f\"Categorical Columns being used ({len(active_categorical_cols)}): {active_categorical_cols}\")\n",
        "\n",
        "        # IMPORTANT: Overwrite the global list with the detected columns before returning\n",
        "        CATEGORICAL_COLUMNS = active_categorical_cols\n",
        "\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_and_save_model(data: pd.DataFrame, model_path: str):\n",
        "    \"\"\"Trains a new CTGAN model and saves it to a pickle file.\"\"\"\n",
        "    # CATEGORICAL_COLUMNS is now updated globally in load_data\n",
        "\n",
        "    print(\"Starting CTGAN model training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize CTGAN model\n",
        "    model = CTGAN(\n",
        "        epochs=300,\n",
        "        batch_size=500,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Fit the model to the real data\n",
        "    model.fit(data, CATEGORICAL_COLUMNS)\n",
        "\n",
        "    # Save the trained model\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Model training complete and saved to {model_path}.\")\n",
        "    print(f\"Total training time: {end_time - start_time:.2f} seconds.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_model(model_path: str):\n",
        "    \"\"\"Loads a previously trained CTGAN model.\"\"\"\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"ERROR: Model file not found at {model_path}.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading trained model from {model_path}...\")\n",
        "    try:\n",
        "        with open(model_path, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        print(\"Model loaded successfully.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading model: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_synthetic_data(model: CTGAN, num_samples: int, output_path: str):\n",
        "    \"\"\"Generates synthetic data and saves it to a CSV file.\"\"\"\n",
        "    print(f\"Generating {num_samples} synthetic samples...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    synthetic_data = model.sample(num_samples)\n",
        "\n",
        "    # Save the synthetic data\n",
        "    synthetic_data.to_csv(output_path, index=False)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Synthetic data generated and saved to {output_path}.\")\n",
        "    print(f\"Generation time: {end_time - start_time:.2f} seconds.\")\n",
        "    print(synthetic_data.head())\n",
        "\n",
        "\n",
        "def run_automation():\n",
        "    \"\"\"Main function to check conditions and execute the pipeline.\"\"\"\n",
        "\n",
        "    data = load_data(INPUT_DATA_FILE)\n",
        "    if data is None:\n",
        "        return\n",
        "\n",
        "    # Determine if retraining is needed\n",
        "    retrain_needed = False\n",
        "\n",
        "    # If the file exists, we check if retraining is necessary based on file modification times.\n",
        "    if not os.path.exists(MODEL_FILE):\n",
        "        print(\"ACTION: Model file does not exist. Retraining is required.\")\n",
        "        retrain_needed = True\n",
        "    else:\n",
        "        try:\n",
        "            # Check if the input data file is newer than the model file\n",
        "            input_time = os.path.getmtime(INPUT_DATA_FILE)\n",
        "            model_time = os.path.getmtime(MODEL_FILE)\n",
        "\n",
        "            if input_time > model_time:\n",
        "                print(\"ACTION: Input data is newer than the existing model. Retraining is required.\")\n",
        "                retrain_needed = True\n",
        "            else:\n",
        "                print(\"STATUS: Existing model is up-to-date with the input data. Reusing model.\")\n",
        "        except Exception as e:\n",
        "            # Handle case where file metadata cannot be read (e.g., permissions)\n",
        "            print(f\"WARNING: Could not check file modification times ({e}). Assuming retraining is needed.\")\n",
        "            retrain_needed = True\n",
        "\n",
        "    if retrain_needed:\n",
        "        # Step 1: Train the model on the new data\n",
        "        ctgan_model = train_and_save_model(data, MODEL_FILE)\n",
        "    else:\n",
        "        # Step 1: Load the existing model\n",
        "        ctgan_model = load_model(MODEL_FILE)\n",
        "\n",
        "    if ctgan_model:\n",
        "        # Step 2: Generate synthetic data (generate the same number of rows as the input data)\n",
        "        NUM_SAMPLES = data.shape[0]\n",
        "        generate_synthetic_data(ctgan_model, NUM_SAMPLES, SYNTHETIC_OUTPUT_FILE)\n",
        "        print(\"\\nPipeline complete. Synthetic data is ready.\")\n",
        "    else:\n",
        "        print(\"\\nPipeline failed: Could not load or train the model.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # NOTE: Before running, ensure you have an 'input_data.csv' file in the same directory.\n",
        "    run_automation()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRi-sRZ8v5gl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}